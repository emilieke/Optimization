{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non linear programing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random data, at least m=1000 observations, from a predefined linear regression model with at least n=100 variables. Assume that the regression coefficients ùú∑=(ùõΩ1,‚Ä¶,ùõΩùëñ,‚Ä¶,ùõΩùëõ) are integers where: ‚àí5‚â§ùõΩùëñ‚â§5 ‚àÄùëñ. Assume also normal residuals (ùëí~ùëÅ(0,ùúé))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "m = 1000 ;\n",
    "n = 100;\n",
    "sigma = 10; # Only a first value\n",
    "\n",
    "X0 = np.ones([m,1])\n",
    "#np.random.seed(0)\n",
    "X1 = np.random.uniform(0,10, ([m,n]))\n",
    "X = np.concatenate([X0,X1],axis = 1)\n",
    "np.savetxt(\"VariableX.txt\",X)\n",
    "error = np.random.normal (0,sigma,(m,1))\n",
    "beta = np.random.randint(-5, 5, size= ([n+1,1]))\n",
    "Y = np.dot(X,beta)+error\n",
    "np.savetxt(\"VariableY.txt\",Y)\n",
    "#print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Estimate the value of the regression coefficients by using the analytical solution for the least squares estimation problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-12.80148123]\n",
      " [  2.02315799]\n",
      " [ -2.83327817]\n",
      " [  1.10931473]\n",
      " [ -4.00819808]\n",
      " [  3.21362022]\n",
      " [ -2.80823993]\n",
      " [ -3.15944649]\n",
      " [ -1.82769344]\n",
      " [ -0.03478562]\n",
      " [  3.07342417]\n",
      " [  2.10272571]\n",
      " [  3.21230504]\n",
      " [ -0.98493778]\n",
      " [ -2.92192449]\n",
      " [  4.00635617]\n",
      " [ -4.14285432]\n",
      " [ -1.70654683]\n",
      " [  3.96672877]\n",
      " [  4.06179884]\n",
      " [ -3.94814221]\n",
      " [  3.98089193]\n",
      " [  1.0521138 ]\n",
      " [  0.1321263 ]\n",
      " [ -1.0493243 ]\n",
      " [  2.09933889]\n",
      " [  0.79615279]\n",
      " [  1.89999252]\n",
      " [ -1.90755422]\n",
      " [ -4.24067259]\n",
      " [  3.88943055]\n",
      " [ -3.0990061 ]\n",
      " [ -4.14332935]\n",
      " [ -3.00697585]\n",
      " [  2.95442801]\n",
      " [ -3.05961079]\n",
      " [  0.86957953]\n",
      " [  0.95713132]\n",
      " [ -4.03168188]\n",
      " [ -3.84221915]\n",
      " [ -2.04394447]\n",
      " [  0.04496062]\n",
      " [  0.12712519]\n",
      " [  2.04347987]\n",
      " [ -4.00887338]\n",
      " [ -2.01530972]\n",
      " [ -4.98352612]\n",
      " [  1.06935986]\n",
      " [ -1.97742574]\n",
      " [  3.95021791]\n",
      " [ -3.92781882]\n",
      " [  3.91865661]\n",
      " [ -3.10784065]\n",
      " [  1.02420172]\n",
      " [  2.11132248]\n",
      " [  2.04151894]\n",
      " [ -4.82423611]\n",
      " [  4.07516065]\n",
      " [ -3.97528029]\n",
      " [ -2.99810971]\n",
      " [ -2.13874614]\n",
      " [  2.2176153 ]\n",
      " [ -0.02800426]\n",
      " [ -0.94524787]\n",
      " [  4.01645442]\n",
      " [ -0.86632704]\n",
      " [ -0.02989609]\n",
      " [ -0.92099833]\n",
      " [  0.99296   ]\n",
      " [  3.95158801]\n",
      " [  0.20476381]\n",
      " [ -0.14456871]\n",
      " [ -4.0581105 ]\n",
      " [  1.99853113]\n",
      " [  0.91213212]\n",
      " [  0.94121568]\n",
      " [ -2.88712079]\n",
      " [  0.83036847]\n",
      " [ -2.00695228]\n",
      " [ -4.90572314]\n",
      " [  0.09827711]\n",
      " [ -2.20030649]\n",
      " [ -0.05329113]\n",
      " [ -3.13435415]\n",
      " [ -4.96530751]\n",
      " [ -5.01477747]\n",
      " [ -2.90895637]\n",
      " [ -4.12808116]\n",
      " [  1.0699397 ]\n",
      " [ -3.92297972]\n",
      " [ -5.11708124]\n",
      " [ -0.84387479]\n",
      " [ -0.78684143]\n",
      " [  2.14441244]\n",
      " [ -3.99651843]\n",
      " [ -0.78656387]\n",
      " [ -3.91058085]\n",
      " [  0.93690182]\n",
      " [  3.86801893]\n",
      " [ -2.97066688]\n",
      " [ -2.04489054]]\n"
     ]
    }
   ],
   "source": [
    "beta_ls = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X),X)),np.transpose(X)),Y)\n",
    "print(beta_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Let ùú∑ÃÇ=(ùõΩÃÇ1,‚Ä¶,ùõΩÃÇùëñ,‚Ä¶,ùõΩÃÇùëõ) be the coefficient' estimations obtained in a) and ùú∑=(ùõΩ1,‚Ä¶,ùõΩùëñ,‚Ä¶,ùõΩùëõ) be their real value used to generate the data. We can define a measure ùúÜ of the estimation error as:\n",
    "\n",
    "ùúÜ=$\\frac{‚Äñùú∑‚àíùú∑ÃÇ‚Äñ}{‚Äñùú∑‚Äñ}√ó100$\n",
    "\n",
    "Analyze how the estimation error (ùúÜ) varies with ùúé (standard deviation assumed for the residuals). Make use of a graph to illustrate this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DECLARE LIST THAT WILL LAMBDA VALUES\n",
    "EstimError= [];\n",
    "\n",
    "#DEFINE RANGE FOR SIGMA\n",
    "sigma_values = np.arange(0.5,10000, 20) #from 0.5 to 30 in steps of 0.25\n",
    "\n",
    "for sigma in sigma_values:\n",
    "    Aux_error = np.random.normal (0,sigma,(m,1)) #generate error with normal distribution\n",
    "    Y_aux = np.dot(X,beta)+Aux_error                 #generate new prediction\n",
    "    beta_ls_aux = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X),X)),\n",
    "                            np.transpose(X)),Y_aux) #Estimate Betas using LSE\n",
    "    #Use 2-norm to estimate the error\n",
    "    EstimError.append(np.linalg.norm(np.matrix(beta)-beta_ls_aux,2)/np.linalg.norm(beta,2)*100) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(sigma_values,EstimError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Estimate the value of the regression coefficients (least squares) by using the tool minimize from the python package Scipy.optimize. Try at least three available solvers and compare their performance (iterations, function, gradient and hessian evaluations as well as total computational time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLSQP Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import time\n",
    "\n",
    "# Defining the function to minimize\n",
    "def squares(beta,X, Y):\n",
    "    return sum((Y[i]-np.dot(beta,X[i,].T))**2 for i in range(0,m)) \n",
    "\n",
    "#Create bounds (-5,5)\n",
    "bnds = []\n",
    "for i in range(0,n+1):\n",
    "    bnds.append((-5,5))\n",
    "    \n",
    "#Initial State of the Betas\n",
    "beta0 = np.zeros(n+1) #Initial state \n",
    "\n",
    "#************** SLSQP SOLVER **********************************\n",
    "start = time.time()\n",
    "beta_SLSQP = minimize(squares,\n",
    "                  beta0,\n",
    "                  args=(X, Y),\n",
    "                  method='SLSQP',\n",
    "                  bounds=bnds,\n",
    "                  options={'disp': False,'ftol': 1e-15})\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution time of SLSQP is: 31.7850\n",
      "Number of iterations: 101\n",
      "Number of evaluations of Obj. Function: 10598\n",
      "Final value of Obj. Function: [ 98208.11433934]\n",
      "Number of evaluations of Gradient: 101\n",
      "Hessian not used\n",
      "error = 5.6763\n"
     ]
    }
   ],
   "source": [
    "#Print Results\n",
    "print '\\nExecution time of SLSQP is: {0:.4f}'.format(end - start)\n",
    "print 'Number of iterations: {0}'.format(beta_SLSQP.nit)\n",
    "print 'Number of evaluations of Obj. Function: {0}'.format(beta_SLSQP.nfev)\n",
    "print 'Final value of Obj. Function: {0!s}'.format(beta_SLSQP.fun)\n",
    "if 'njev' in beta_SLSQP.keys():\n",
    "    print 'Number of evaluations of Gradient: {0}'.format(beta_SLSQP.njev)\n",
    "else:\n",
    "    print 'Gradient not used' \n",
    "\n",
    "if 'nhev' in beta_SLSQP.keys():\n",
    "    print 'Number of evaluations of Hessian: {0}'.format(beta_SLSQP.nhev)\n",
    "else:\n",
    "    print 'Hessian not used' \n",
    "print 'error = {0:.4f}'.format(np.linalg.norm(np.matrix(beta)-np.transpose(np.matrix(beta_SLSQP.x)),2)/np.linalg.norm(beta,2)*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-BFGS-B Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#************** L-BFGS-B SOLVER **********************************\n",
    "start = time.time()\n",
    "beta_LBFGSB = minimize(squares,\n",
    "                  beta0,\n",
    "                  args=(X, Y),\n",
    "                  method='L-BFGS-B',\n",
    "                  bounds=bnds,\n",
    "                  options={'disp': False,'ftol': 1e-15})\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution time of L-BFGS-B is: 44.2560\n",
      "Number of iterations: 120\n",
      "Number of evaluations of Obj. Function: 15198\n",
      "Final value of Obj. Function: [ 86238.61368306]\n",
      "Gradient not used\n",
      "Hessian not used\n",
      "error = 5.7796\n"
     ]
    }
   ],
   "source": [
    "#Print Results\n",
    "print '\\nExecution time of L-BFGS-B is: {0:.4f}'.format(end - start)\n",
    "print 'Number of iterations: {0}'.format(beta_LBFGSB.nit)\n",
    "print 'Number of evaluations of Obj. Function: {0}'.format(beta_LBFGSB.nfev)\n",
    "print 'Final value of Obj. Function: {0!s}'.format(beta_LBFGSB.fun)\n",
    "if 'njev' in beta_LBFGSB.keys():\n",
    "    print 'Number of evaluations of Gradient: {0}'.format(beta_LBFGSB.njev)\n",
    "else:\n",
    "    print 'Gradient not used' \n",
    "\n",
    "if 'nhev' in beta_LBFGSB.keys():\n",
    "    print 'Number of evaluations of Hessian: {0}'.format(beta_LBFGSB.nhev)\n",
    "else:\n",
    "    print 'Hessian not used'  \n",
    "print 'error = {0:.4f}'.format(np.linalg.norm(np.matrix(beta)-np.transpose(np.matrix(beta_LBFGSB.x)),2)/np.linalg.norm(beta,2)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TNC Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#************** TNC SOLVER **********************************\n",
    "start = time.time()\n",
    "beta_TNC = minimize(squares,\n",
    "                  beta0,\n",
    "                  args=(X, Y),\n",
    "                  method='TNC',\n",
    "                  bounds=bnds,\n",
    "                  options={'disp': False,'ftol': 1e-15})\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution time of L-BFGS-B is: 315.4010\n",
      "Number of iterations: 101\n",
      "Number of evaluations of Obj. Function: 1010\n",
      "Final value of Obj. Function: [ 248210.01265036]\n",
      "Gradient not used\n",
      "Hessian not used\n",
      "error = 22.9045\n"
     ]
    }
   ],
   "source": [
    "#Print Results\n",
    "print '\\nExecution time of L-BFGS-B is: {0:.4f}'.format(end - start)\n",
    "print 'Number of iterations: {0}'.format(beta_TNC.nit)\n",
    "print 'Number of evaluations of Obj. Function: {0}'.format(beta_TNC.nfev)\n",
    "print 'Final value of Obj. Function: {0!s}'.format(beta_TNC.fun)\n",
    "if 'njev' in beta_TNC.keys():\n",
    "    print 'Number of evaluations of Gradient: {0}'.format(beta_TNC.njev)\n",
    "else:\n",
    "    print 'Gradient not used' \n",
    "\n",
    "if 'nhev' in beta_TNC.keys():\n",
    "    print 'Number of evaluations of Hessian: {0}'.format(beta_TNC.nhev)\n",
    "else:\n",
    "    print 'Hessian not used'  \n",
    "print 'error = {0:.4f}'.format(np.linalg.norm(np.matrix(beta)-np.transpose(np.matrix(beta_TNC.x)),2)/np.linalg.norm(beta,2)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Considering again the least squares estimation problem, estimate the value of the regression coefficients\n",
    "by implementing the:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ Definition objective function, gradient and Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#definitinition of OF\n",
    "def least_sq_reg(beta_ls, X, Y):\n",
    "    beta_ls=np.matrix(beta_ls)\n",
    "    z=Y-X*np.transpose(beta_ls)\n",
    "    return np.transpose(z)*z\n",
    "\n",
    "#definition of Gradient\n",
    "def least_sq_reg_der(beta_ls,X,Y):\n",
    "    beta_ls=np.matrix(beta_ls)\n",
    "    pp=-2*np.transpose(Y-X*np.transpose(beta_ls))*X\n",
    "    aa= np.squeeze(np.asarray(pp))\n",
    "    return aa\n",
    "\n",
    "#definition of hessian\n",
    "def least_sq_reg_hess(beta_ls,X,Y):\n",
    "    ss=2*np.dot(np.transpose(X),X)\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(a,b)=X.shape\n",
    "beta_lsg=np.zeros(b) #initial value for beta\n",
    "n_iter=100000 #maximim nunber iteration\n",
    "OF_iter=np.zeros(n_iter)\n",
    "tol_iter=np.zeros(n_iter)\n",
    "alpha_iter=np.zeros(n_iter)\n",
    "i=0;\n",
    "tol=1000;\n",
    "epsilon=1e-3;\n",
    "\n",
    "time_start = time.clock()\n",
    "while (i <= n_iter-2) and (tol>epsilon):\n",
    "    i=i+1\n",
    "    grad = least_sq_reg_der(beta_lsg,X,Y)\n",
    "    ddirect = - grad\n",
    "    ###################################\n",
    "    #      Armijo Rule-----Choose an appropiate alpha\n",
    "    sigma = 0.1\n",
    "    beta_half = 0.5\n",
    "    alpha = 1\n",
    "    while (least_sq_reg(beta_lsg + alpha*ddirect , X, Y)> least_sq_reg(beta_lsg, X, Y) + alpha*sigma*np.dot(grad,ddirect)):\n",
    "        alpha = alpha*beta_half\n",
    "    ###################################\n",
    "    beta_lsg = beta_lsg + alpha*ddirect\n",
    "    OF_iter[i] = least_sq_reg(beta_lsg, X, Y)\n",
    "    tol = np.linalg.norm(grad,ord=2)\n",
    "    tol_iter[i] = tol\n",
    "    alpha_iter[i] = alpha\n",
    "time_elapsed = (time.clock() - time_start)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution time of Gradient Method is: 180.9180\n",
      "Number of iterations: 99999\n",
      "Number of evaluations of Obj. Function: 99999\n",
      "Final value of Obj. Function: 85442.060907\n",
      "Number of evaluations of Gradient: 99999\n",
      "Hessian not used\n",
      "error = 15.0492\n"
     ]
    }
   ],
   "source": [
    "#Print Results\n",
    "print '\\nExecution time of Gradient Method is: {0:.4f}'.format(time_elapsed)\n",
    "print 'Number of iterations: {0}'.format(i)\n",
    "print 'Number of evaluations of Obj. Function: {0}'.format(i)\n",
    "print 'Final value of Obj. Function: {0:4f}'.format(OF_iter[i])\n",
    "print 'Number of evaluations of Gradient: {0}'.format(i)\n",
    "print 'Hessian not used'\n",
    "print 'error = {0:.4f}'.format(np.linalg.norm(np.matrix(beta)-np.transpose(np.matrix(beta_lsg)),2)/np.linalg.norm(beta,2)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(a,b)=X.shape\n",
    "beta_newton=np.zeros(b) #initial value for beta\n",
    "n_iter=100000 #maximim nunber iteration\n",
    "OF_iter=np.zeros(n_iter)\n",
    "tol_iter=np.zeros(n_iter)\n",
    "alpha_iter=np.zeros(n_iter)\n",
    "alpha_iter=np.zeros(n_iter)\n",
    "i=0;\n",
    "tol=1000;\n",
    "epsilon=1e-8;\n",
    "\n",
    "time_start = time.clock()\n",
    "while (i <= n_iter-2) and (tol>epsilon):\n",
    "    i=i+1\n",
    "    grad = least_sq_reg_der(beta_newton,X,Y)\n",
    "    hess = least_sq_reg_hess(beta_newton,X,Y)\n",
    "    ddirect = - np.dot(np.linalg.inv(hess),grad)\n",
    "    ###################################\n",
    "    #      Armijo Rule-----Choose an appropiate alpha\n",
    "    sigma = 0.1\n",
    "    beta_half = 0.5\n",
    "    alpha = 1\n",
    "    while (least_sq_reg(beta_newton + alpha*ddirect , X, Y)> least_sq_reg(beta_newton, X, Y) + alpha*sigma*np.dot(grad,ddirect)):\n",
    "        alpha = alpha*beta_half\n",
    "    ###################################\n",
    "    beta_newton = beta_newton + alpha*ddirect\n",
    "    OF_iter[i] = least_sq_reg(beta_newton, X, Y)\n",
    "    tol = np.linalg.norm(grad,ord=2)\n",
    "    tol_iter[i] = tol\n",
    "    alpha_iter[i] = alpha\n",
    "time_elapsed = (time.clock() - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution time of Newton Method is: 0.0032\n",
      "Number of iterations: 3\n",
      "Number of evaluations of Obj. Function: 3\n",
      "Final value of Obj. Function: 85441.836378\n",
      "Number of evaluations of Gradient: 3\n",
      "Number of evaluations of Hessian: 3\n",
      "error = 15.9964\n"
     ]
    }
   ],
   "source": [
    "#Print Results\n",
    "print '\\nExecution time of Newton Method is: {0:.4f}'.format(time_elapsed)\n",
    "print 'Number of iterations: {0}'.format(i)\n",
    "print 'Number of evaluations of Obj. Function: {0}'.format(i)\n",
    "print 'Final value of Obj. Function: {0:4f}'.format(OF_iter[i])\n",
    "print 'Number of evaluations of Gradient: {0}'.format(i)\n",
    "print 'Number of evaluations of Hessian: {0}'.format(i)\n",
    "print 'error = {0:.4f}'.format(np.linalg.norm(np.matrix(beta)-np.transpose(np.matrix(beta_newton)),2)/np.linalg.norm(beta,2)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quasi-Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(a,b)=X.shape\n",
    "beta_quasi=np.zeros(b) #initial value for beta\n",
    "n_iter=100000 #maximim nunber iteration\n",
    "OF_iter=np.zeros(n_iter)\n",
    "tol_iter=np.zeros(n_iter)\n",
    "alpha_iter=np.zeros(n_iter)\n",
    "alpha_iter=np.zeros(n_iter)\n",
    "i=0;\n",
    "tol=1000;\n",
    "epsilon=1e-8;\n",
    "\n",
    "time_start = time.clock()\n",
    "while (i <= n_iter-2) and (tol>epsilon):\n",
    "    i=i+1\n",
    "    grad = least_sq_reg_der(beta_quasi,X,Y)\n",
    "    if (i==1):\n",
    "        grad = least_sq_reg_der(beta_quasi,X,Y)\n",
    "        B = least_sq_reg_hess(beta_quasi,X,Y)\n",
    "    else:\n",
    "        grad_before = grad\n",
    "        grad = least_sq_reg_der(beta_quasi,X,Y)\n",
    "        y = grad - grad_before\n",
    "        s = beta_quasi - beta_quasi_before\n",
    "        B_before = B\n",
    "        B = B_before + np.dot(y-np.dot(B,s),np.transpose(y-np.dot(B,s)))/(np.dot(np.transpose(y-np.dot(B,s)),s))\n",
    "    ddirect = - np.dot(np.linalg.inv(B),grad)\n",
    "    ###################################\n",
    "    #      Armijo Rule-----Choose an appropiate alpha\n",
    "    sigma = 0.1\n",
    "    beta_half = 0.5\n",
    "    alpha = 1\n",
    "    while (least_sq_reg(beta_quasi + alpha*ddirect , X, Y)> least_sq_reg(beta_quasi, X, Y) + alpha*sigma*np.dot(grad,ddirect)):\n",
    "        alpha = alpha*beta_half\n",
    "    ###################################\n",
    "    beta_quasi_before = beta_quasi\n",
    "    beta_quasi = beta_quasi_before + alpha*ddirect\n",
    "    OF_iter[i] = least_sq_reg(beta_quasi, X, Y)\n",
    "    tol = np.linalg.norm(grad,ord=2)\n",
    "    tol_iter[i] = tol\n",
    "    alpha_iter[i] = alpha   \n",
    "time_elapsed = (time.clock() - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution time of Quasi-Newton Method is: 0.0032\n",
      "Number of iterations: 3\n",
      "Number of evaluations of Obj. Function: 3\n",
      "Final value of Obj. Function: 85441.836378\n",
      "Number of evaluations of Gradient: 3\n",
      "Number of evaluations of Hessian: 1\n",
      "error = 15.9964\n"
     ]
    }
   ],
   "source": [
    "#Print Results\n",
    "print '\\nExecution time of Quasi-Newton Method is: {0:.4f}'.format(time_elapsed)\n",
    "print 'Number of iterations: {0}'.format(i)\n",
    "print 'Number of evaluations of Obj. Function: {0}'.format(i)\n",
    "print 'Final value of Obj. Function: {0:4f}'.format(OF_iter[i])\n",
    "print 'Number of evaluations of Gradient: {0}'.format(i)\n",
    "print 'Number of evaluations of Hessian: {0}'.format(1)\n",
    "print 'error = {0:.4f}'.format(np.linalg.norm(np.matrix(beta)-np.transpose(np.matrix(beta_quasi)),2)/np.linalg.norm(beta,2)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Estimate the value of the regression coefficients y implementing the coordinate gradient method and the stochastic gradient method. Compare their performance with the algorithms in c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fd0c094ddc3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#alpha = 0.00001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbeta_coor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#initial value for beta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m \u001b[0;31m#maximim nunber iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import random \n",
    "(a,b)=X.shape\n",
    "#alpha = 0.00001\n",
    "beta_coor=np.zeros(b) #initial value for beta\n",
    "n_iter=100000 #maximim nunber iteration\n",
    "OF_iter=np.zeros(n_iter)\n",
    "tol_iter=np.zeros(n_iter)\n",
    "alpha_iter=np.zeros(n_iter)\n",
    "i=0;\n",
    "tol=1000;\n",
    "epsilon=1e-3;\n",
    "\n",
    "time_start = time.clock()\n",
    "while (i <= n_iter-2) and (tol>epsilon):\n",
    "    i=i+1\n",
    "    grad = least_sq_reg_der(beta_coor,X,Y)\n",
    "    ddirect =np.zeros(b)\n",
    "    j = random.randint(0, b-1)\n",
    "    ddirect[j] = - grad[j]\n",
    "    print(grad[j])\n",
    "    ###################################\n",
    "    #      Armijo Rule-----Choose an appropiate alpha\n",
    "    sigma = 0.1\n",
    "    beta_half = 0.5\n",
    "    alpha = 1\n",
    "    while (least_sq_reg(beta_coor + alpha*ddirect , X, Y)> least_sq_reg(beta_coor, X, Y) + alpha*sigma*np.dot(grad,ddirect)):\n",
    "          alpha = alpha*beta_half\n",
    "    ###################################\n",
    "    beta_coor = beta_coor + alpha*ddirect\n",
    "    OF_iter[i] = least_sq_reg(beta_coor, X, Y)\n",
    "    tol = np.linalg.norm(grad,ord=2)\n",
    "    tol_iter[i] = tol\n",
    "    alpha_iter[i] = alpha\n",
    "\n",
    "time_elapsed = (time.clock() - time_start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution time of Coordinate Gradient Method is: 130.7382\n",
      "Number of iterations: 99999\n",
      "Number of evaluations of Obj. Function: 99999\n",
      "Final value of Obj. Function: 85441.845101\n",
      "Number of evaluations of Gradient: 99999\n",
      "Hessian not used\n",
      "error = 15.8094\n"
     ]
    }
   ],
   "source": [
    "#Print Results\n",
    "print '\\nExecution time of Coordinate Gradient Method is: {0:.4f}'.format(time_elapsed)\n",
    "print 'Number of iterations: {0}'.format(i)\n",
    "print 'Number of evaluations of Obj. Function: {0}'.format(i)\n",
    "print 'Final value of Obj. Function: {0:4f}'.format(OF_iter[i])\n",
    "print 'Number of evaluations of Gradient: {0}'.format(i)\n",
    "print 'Hessian not used'\n",
    "print 'error = {0:.4f}'.format(np.linalg.norm(np.matrix(beta)-np.transpose(np.matrix(beta_coor)),2)/np.linalg.norm(beta,2)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#definition of partial gradient \n",
    "def least_sq_reg_der_par(beta_ls,X,Y,i):\n",
    "    beta_ls=np.matrix(beta_ls)\n",
    "    pp=-2*(Y[i]-X[i,]*np.transpose(beta_ls))*X[i,]\n",
    "    aa= np.squeeze(np.asarray(pp))\n",
    "    return aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "(a,b)=X.shape\n",
    "#alpha = 0.0001\n",
    "beta_sto=np.zeros(b) #initial value for beta\n",
    "n_iter=100000 #maximum nunber iteration\n",
    "OF_iter=np.zeros(n_iter)\n",
    "tol_iter=np.zeros(n_iter)\n",
    "alpha_iter=np.zeros(n_iter)\n",
    "i=0;\n",
    "tol=1000;\n",
    "epsilon=1e-3;\n",
    "\n",
    "time_start = time.clock()\n",
    "while (i <= n_iter-2) and (tol>epsilon):\n",
    "    i=i+1\n",
    "    j = random.randint(0, b-1)\n",
    "    grad_par = least_sq_reg_der_par(beta_sto,X,Y,j)\n",
    "    grad = least_sq_reg_der(beta_sto,X,Y)\n",
    "    ddirect = - grad_par\n",
    "    ###################################\n",
    "    #      Armijo Rule-----Choose an appropiate alpha\n",
    "    sigma = 0.1\n",
    "    beta_half = 0.5\n",
    "    alpha = 1\n",
    "    while (least_sq_reg(beta_sto + alpha*ddirect , X, Y)> least_sq_reg(beta_sto, X, Y) + alpha*sigma*np.dot(grad,ddirect)):\n",
    "           alpha = alpha*beta_half\n",
    "    ###################################\n",
    "    beta_sto = beta_sto + ddirect*alpha\n",
    "    OF_iter[i] = least_sq_reg(beta_sto, X, Y)\n",
    "    tol = np.linalg.norm(grad,ord=2)\n",
    "    tol_iter[i] = tol\n",
    "    alpha_iter[i] = alpha\n",
    "time_elapsed = (time.clock() - time_start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution time of Stochastic Gradient Method is: 257.0573\n",
      "Number of iterations: 99999\n",
      "Number of evaluations of Obj. Function: 99999\n",
      "Final value of Obj. Function: 1834117.258382\n",
      "Number of evaluations of Gradient: 99999\n",
      "Hessian not used\n",
      "error = 56.4157\n"
     ]
    }
   ],
   "source": [
    "#Print Results\n",
    "print '\\nExecution time of Stochastic Gradient Method is: {0:.4f}'.format(time_elapsed)\n",
    "print 'Number of iterations: {0}'.format(i)\n",
    "print 'Number of evaluations of Obj. Function: {0}'.format(i)\n",
    "print 'Final value of Obj. Function: {0:4f}'.format(OF_iter[i])\n",
    "print 'Number of evaluations of Gradient: {0}'.format(i)\n",
    "print 'Hessian not used'\n",
    "print 'error = {0:.4f}'.format(np.linalg.norm(np.matrix(beta)-np.transpose(np.matrix(beta_sto)),2)/np.linalg.norm(beta,2)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1834117.25838171]]\n"
     ]
    }
   ],
   "source": [
    "print(least_sq_reg(beta_sto, X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
